{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tables\n",
    "from tables import IsDescription, StringCol, Float32Col, Float64Col\n",
    "import glob\n",
    "# import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_distribution(img, blocks, nbins):\n",
    "    edges = cv2.Canny(img, threshold1=50, threshold2=120)\n",
    "    \n",
    "    smoothed = cv2.GaussianBlur(gray_img, ksize=(3,3), sigmaX=0, sigmaY=0)\n",
    "    dx = cv2.Sobel(smoothed, cv2.CV_16S, ksize=3, dx=1, dy=0)\n",
    "    dy = cv2.Sobel(smoothed, cv2.CV_32F, ksize=3, dx=0, dy=1)\n",
    "    theta = np.arctan2(dy, dx)\n",
    "    \n",
    "    ysize, xsize = img.shape\n",
    "    yblocks, xblocks = blocks\n",
    "    xstep = math.ceil(xsize / xblocks)\n",
    "    ystep = math.ceil(ysize / yblocks)\n",
    "    if xstep*(xblocks-1) >= xsize or ystep*(yblocks-1) >= ysize:\n",
    "        raise Exception('The image of shape %s is not suitable for blocks of %s.' % (img.shape, blocks))\n",
    "\n",
    "    masks = []\n",
    "    for i in range(xblocks):\n",
    "        x = np.zeros(xsize)\n",
    "        x[i*xstep:min((i+1)*xstep, xsize)] = 1\n",
    "        for j in range(yblocks):\n",
    "            y = np.zeros(ysize)\n",
    "            y[j*ystep:min((j+1)*ystep, ysize)] = 1\n",
    "\n",
    "            mask = np.outer(y, x)\n",
    "            masks += [mask]\n",
    "    \n",
    "    counters = []\n",
    "    bins = np.append(np.arange(-np.pi, np.pi, 2*np.pi/nbins), np.pi)\n",
    "    for b in range(len(masks)):\n",
    "        for i in range(nbins):\n",
    "            theta_mask = (theta >= bins[i]) & (theta < bins[i+1])\n",
    "            edges0 = theta_mask * masks[b] * edges\n",
    "            counters += [np.sum(edges0)]\n",
    "            \n",
    "        counters += [np.sum(masks[b] * edges) / np.sum(edges) if np.sum(edges) > 0 else 0]\n",
    "        \n",
    "    return counters\n",
    "\n",
    "def gray_averages(img, blocks):\n",
    "    ysize, xsize = img.shape\n",
    "    yblocks, xblocks = blocks\n",
    "    xstep = math.ceil(xsize / xblocks)\n",
    "    ystep = math.ceil(ysize / yblocks)\n",
    "    if xstep*(xblocks-1) >= xsize or ystep*(yblocks-1) >= ysize:\n",
    "        raise Exception('The image of shape %s is not suitable for blocks of %s.' % (img.shape, blocks))\n",
    "\n",
    "    masks = []\n",
    "    for i in range(xblocks):\n",
    "        x = np.zeros(xsize)\n",
    "        x[i*xstep:min((i+1)*xstep, xsize)] = 1\n",
    "        for j in range(yblocks):\n",
    "            y = np.zeros(ysize)\n",
    "            y[j*ystep:min((j+1)*ystep, ysize)] = 1\n",
    "\n",
    "            mask = np.outer(y, x)\n",
    "            masks += [mask]\n",
    "            \n",
    "    avg = []\n",
    "    for b in range(len(masks)):\n",
    "        avg += [np.sum(gray_img * masks[b]) / np.sum(masks[b])]\n",
    "        \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = glob.glob('./**/*.jpg', recursive=True)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = img_glob_feat_table.row\n",
    "for p in img_list:\n",
    "    i = p.rfind('/')\n",
    "    h = p[i+1:]\n",
    "#     if len(h) != 32:\n",
    "#         print('Not good file.')\n",
    "#         continue\n",
    "    gray_img = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2GRAY)\n",
    "    feat0 = edge_distribution(gray_img, (2,2), 12)\n",
    "    feat1 = gray_averages(gray_img, (8,8))\n",
    "    feat = feat0 + feat1\n",
    "    \n",
    "    row['hash'] = h\n",
    "    row['glob_feat'] = feat\n",
    "    row.append()\n",
    "    \n",
    "img_glob_feat_table.flush()\n",
    "raw_feat_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce raw global features and fit PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawGlobFeature(IsDescription):\n",
    "    hash = StringCol(48)\n",
    "    glob_feat  = Float64Col(116)\n",
    "\n",
    "feat_db = tables.open_file(\"./dup_img_feat.h5\", mode=\"a\", title=\"Features for duplicate detection\")\n",
    "group = feat_db.create_group(\"/\", 'default', 'the default group')\n",
    "raw_glob_feat_table = feat_db.create_table(group, 'raw_glob_feat', RawGlobFeature, 'Raw global features of images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_pca = IncrementalPCA(n_components=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(img_list)\n",
    "bsize = 256\n",
    "row = img_glob_feat_table.row\n",
    "glob_feat_avg = np.zeros(116)\n",
    "for i in range(math.ceil(n/bsize)):\n",
    "    batch = []\n",
    "    for p in img_list[i*bsize:min((i+1)*bsize, n)]:\n",
    "        try:\n",
    "            gray_img = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2GRAY)\n",
    "            feat0 = edge_distribution(gray_img, (2,2), 12)\n",
    "            feat1 = gray_averages(gray_img, (8,8))\n",
    "            feat = feat0 + feat1\n",
    "            batch += [feat]\n",
    "            \n",
    "            glob_feat_avg += feat\n",
    "            j = p.rfind('/')\n",
    "            row['hash'] = p[j+1:j+33]\n",
    "            row['glob_feat'] = feat\n",
    "            row.append()\n",
    "        except Exception as e:\n",
    "            print('Discard the image due to: %s' % str(e))\n",
    "    if len(batch) < bsize:\n",
    "        continue\n",
    "    batch = np.array(batch)\n",
    "    print(batch.shape)\n",
    "    inc_pca.partial_fit(batch)\n",
    "\n",
    "glob_feat_avg /= n\n",
    "row['hash'] = 'avgxxx'\n",
    "row['glob_feat'] = glob_feat_avg\n",
    "row.append()\n",
    "\n",
    "raw_glob_feat_table.flush()\n",
    "feat_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply PCA on raw lobal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAedGlobFeature(IsDescription):\n",
    "    hash = StringCol(48)\n",
    "    pcaed_glob_feat  = Float64Col(24)\n",
    "\n",
    "feat_db = tables.open_file(\"./dup_img_feat.h5\", mode=\"a\", title=\"Features for duplicate detection\")\n",
    "raw_glob_feat_table = feat_db.root.group.raw_glob_feat_table\n",
    "pcaed_glob_feat_table = feat_db.create_table(feat_db.root.group, 'pcaed_glob_feat', PCAedGlobFeature, 'PCAed global features of images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_thld = raw_glob_feat_table.where('hash==b\"avgxxx\"')  #Warning: Use b prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = pcaed_glob_feat_table.row\n",
    "for r in raw_glob_feat_table.iterrows():\n",
    "    feat = r['glob_feat']\n",
    "    f = inc_pca.transform(feat)\n",
    "    row['hash'] = r['hash']\n",
    "    row['pcaed_glob_feat'] = f\n",
    "    row.append()\n",
    "    \n",
    "pcaed_glob_feat_table.flush()\n",
    "feat_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_dist(f0, f1):\n",
    "    return 0\n",
    "\n",
    "def hamming_dist(sig0, sig1):\n",
    "    return np.sum(sig0 != sig1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_db = tables.open_file(\"./dup_img_feat.h5\", mode=\"r\", title=\"Features for duplicate detection\")\n",
    "raw_glob_feat_table = feat_db.root.group.raw_glob_feat_table\n",
    "pcaed_glob_feat_table = feat_db.root.group.pcaed_glob_feat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters = {} # cluster_name: [hash1, hash2]\n",
    "buckets = {} # bucket_sig: {points:[hash1, hash2], clusters:[cluster_name1]}\n",
    "points = {} # hash: {cluster: cluster_name, bucket: bucket_sig, glob_feat: []}\n",
    "\n",
    "eps = 1e-3\n",
    "\n",
    "# n = pcaed_glob_feat_table.nrows\n",
    "for r in pcaed_glob_feat_table.iterrows():\n",
    "    h = r['hash']\n",
    "    points[h] = {}\n",
    "    p = points[h]\n",
    "    p['cluster'] = 'c-' + h\n",
    "    clusters[p['cluster']] = [h]\n",
    "    \n",
    "    p['glob_feat'] = r['pcaed_glob_feat']\n",
    "    p['bucket'] = (np.cap['glob_feat'] > glob_feat_avg).astype(np.int32)\n",
    "    \n",
    "    if p['bucket'] in buckets:\n",
    "        buckets[p['bucket']]['points'] += [h]\n",
    "        buckets[p['bucket']]['clusters'] += [p['cluster']]\n",
    "    else:\n",
    "        buckets[p['bucket']] = {}\n",
    "        buckets[p['bucket']]['points'] = [h]\n",
    "        buckets[p['bucket']]['clusters'] = [p['cluster']]\n",
    "    \n",
    "    \n",
    "\n",
    "# Clustering within each bucket\n",
    "for _, buck in buckets:\n",
    "    hashes = buck['points']\n",
    "    n = len(hashes)\n",
    "    for i in range(n):\n",
    "        hi = hashes[i]\n",
    "        pi = points[hi]\n",
    "        for j in range(i+1,n):\n",
    "            hj = hashes[j]\n",
    "            pj = points[hj]\n",
    "            d = vec_dist(pi['glob_feat'], pj['glob_feat'])  #TODO\n",
    "            if d < eps:  # close enough, merge cj to ci.\n",
    "                pj['cluster'] = pi['cluster']\n",
    "                clusters[pi['cluster']] = clusters[pi['cluster']] + clusters[pj['cluster']]\n",
    "                # Remove cj.\n",
    "                del clusters[pj['cluster']]\n",
    "                buck['clusters'].remove(pj['cluster'])\n",
    "        \n",
    "\n",
    "# Merge clusters across buckets\n",
    "buk_sigs = buckets.keys()\n",
    "for i, sig0 in enumerate(buk_sigs):\n",
    "    for _, sig1 in enumerate(buk_sigs[i+1:]):\n",
    "        diff = hamming_dist(sig0, sig1)\n",
    "        if diff > 2: # Too far away and do not merge\n",
    "            continue\n",
    "        for c0_name in buckets[sig0]['clusters']:\n",
    "            for c1_name in buckets[sig1]['clusters']:\n",
    "                c0_hashes = clusters[c0_name]\n",
    "                c1_hashes = clusters[c1_name]\n",
    "                h0 = c0_hashes[np.random.randint(len(c0_hashes))]\n",
    "                h1 = c1_hashes[np.random.randint(len(c1_hashes))]\n",
    "                if vec_dist(points[h0]['glob_feat'], points[h1]['glob_feat']) < eps: # close enough, merge c1 to c0\n",
    "                    for hh in c1_hashes:\n",
    "                        points[hh]['cluster'] = c0_name\n",
    "                        points[hh]['bucket'] = sig0\n",
    "                        buckets[sig1]['points'].remove[hh]\n",
    "                    clusters[c0_name] = clusters[c0_name] + clusters[c1_name]\n",
    "                    del clusters[c1_name]\n",
    "                    buckets[sig1]['clusters'].remove(c1)\n",
    "            \n",
    "            # The bucket sig1 is empty and removed.\n",
    "            if len(buckets[sig1]['points']) == 0 and len(buckets[sig1]['clusters']) == 0:\n",
    "                del buckets[sig1]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
